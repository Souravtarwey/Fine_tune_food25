{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "9rHhEs97v4xm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload a zip file of trained model available in git repository and provide the correct path."
      ],
      "metadata": {
        "id": "GWCOUre92Twl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the path to your ZIP file\n",
        "zip_file_path = '/content/my_model (1).zip'\n",
        "\n",
        "# Define the directory to extract to\n",
        "extract_path = '/content/my_model/'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# List the contents of the extraction directory\n",
        "os.listdir(extract_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsRpJHfWzT9m",
        "outputId": "9bccf05c-0a1e-47e6-d2b3-4fd449625a28"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['my_model.h5']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the path to the saved model"
      ],
      "metadata": {
        "id": "QyD3KGgf2I9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/my_model/my_model.h5'\n",
        "model = load_model(model_path)"
      ],
      "metadata": {
        "id": "i7_73yQPxizo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Code run this and provide the url of image or path."
      ],
      "metadata": {
        "id": "s09x37Os2LBD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvcAFWwIvzbQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Ensure TensorFlow version compatibility\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Define the path to the saved model\n",
        "\n",
        "# Define the selected classes in sorted order\n",
        "selected_classes = [\n",
        "    'bibimbap', 'breakfast_burrito', 'caesar_salad', 'chicken_quesadilla', 'club_sandwich',\n",
        "    'cup_cakes', 'deviled_eggs', 'dumplings', 'edamame', 'fish_and_chips',\n",
        "    'foie_gras', 'hummus', 'hot_and_sour_soup', 'macaroni_and_cheese', 'nachos',\n",
        "    'omelette', 'samosa', 'sashimi', 'scallops', 'shrimp_and_grits',\n",
        "    'steak', 'tacos', 'takoyaki', 'tuna_tartare', 'waffles'\n",
        "]\n",
        "\n",
        "# Define the image size expected by the model\n",
        "img_size = (224, 224)\n",
        "\n",
        "# Function to preprocess the image\n",
        "def preprocess_image(image_path):\n",
        "    # Load the image\n",
        "    img = load_img(image_path)\n",
        "\n",
        "    # Check image size and resize if necessary\n",
        "    if img.size != img_size:\n",
        "        img = img.resize(img_size)\n",
        "\n",
        "    # Convert image to array\n",
        "    img_array = img_to_array(img)\n",
        "\n",
        "    # Expand dimensions to match the model's input shape\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Normalize the image\n",
        "    img_array /= 255.0\n",
        "\n",
        "    return img_array\n",
        "\n",
        "# Function to classify a single image from user input\n",
        "def classify_single_image(image_path=None):\n",
        "    if image_path is None:\n",
        "        # Running in Google Colab, prompt for image path or URL\n",
        "        print(\"Enter the path or URL of the image to classify:\")\n",
        "        image_path = input().strip()\n",
        "\n",
        "        # If it's a URL, download the image\n",
        "        if image_path.startswith('http://') or image_path.startswith('https://'):\n",
        "            import requests\n",
        "            from PIL import Image\n",
        "            from io import BytesIO\n",
        "\n",
        "            response = requests.get(image_path)\n",
        "            img = Image.open(BytesIO(response.content))\n",
        "            image_path = '/content/uploaded_image.jpg'\n",
        "            img.save(image_path)\n",
        "\n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\"File not found: {image_path}\")\n",
        "        return None\n",
        "\n",
        "    img_array = preprocess_image(image_path)\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class = selected_classes[np.argmax(predictions)]\n",
        "    print(f\"Predicted Class: {predicted_class}\")\n",
        "    return predicted_class\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    classify_single_image()\n"
      ]
    }
  ]
}